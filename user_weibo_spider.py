#!/usr/bin/env python
# -*- coding: utf-8 -*-
#Â @TimeÂ Â Â  : 2020-02-04 17:14
#Â @AuthorÂ  : July
import re
import os
import csv
import time
from retrying import retry
from constants import *
from lxml import html

etree = html.etree

proxy = {}

since_id = '4468349641659375'


@retry(stop_max_attempt_number=3, wait_random_min=1000, wait_random_max=5000)
def change_proxy(retry_count):
    if retry_count < 0:
        return

    result = requests.get(proxy_url).json()
    if result['msg'] == 'ok':
        ip = result['obj'][0]['ip']
        port = result['obj'][0]['port']
        proxies = {"http": "http://" + ip + ":" + port, "https": "http://" + ip + ":" + port}
        global proxy
        proxy = proxies
        print(f"ä»£ç†ipä¸ºæ›´æ”¹ä¸ºï¼š{proxies}")
        return proxies
    else:
        time.sleep(1)
        print('åˆ‡æ¢ä»£ç†å¤±è´¥ï¼Œé‡æ–°å°è¯•ã€‚ã€‚ã€‚')
        change_proxy(retry_count - 1)


def save_data(filename, data):
    if os.path.isfile(filename):
        is_exist = True
    else:
        is_exist = False
    with open(filename, "a", newline="", encoding="utf_8_sig") as f:
        c = csv.writer(f)
        if not is_exist:
            """need = [user_id, user_name, created_at, source,
                    content, reposts_count, comments_count, attitudes_count,
                    pics_url, video_url, retweeted_status, retweeted_url,
                    topic_num, topics]"""
            c.writerow(['ç”¨æˆ·id', 'æ˜µç§°', 'å‘è¡¨æ—¶é—´', 'å‘å¸ƒè®¾å¤‡', 'æ­£æ–‡', 'è½¬å‘æ•°', 'è¯„è®ºæ•°', 'ç‚¹èµžæ•°',
                        'å›¾ç‰‡é“¾æŽ¥', 'è§†é¢‘é“¾æŽ¥', 'æ˜¯å¦è½¬å‘', 'è½¬å‘é“¾æŽ¥', 'è¯é¢˜æ•°', 'è¯é¢˜', 'å¾®åšé“¾æŽ¥'])
        for line in data:
            c.writerow(line)


def spider():
    global since_id
    app_param = {
        'uid': 3097688767,
        'type': 'uid',
        'value': 3097688767,
        'containerid': '1076033097688767',
        'since_id': since_id
    }

    def get_ret(c):
        if c < 0:
            return
        try:
            ret = requests.get(url=app_url, headers=app_header, params=app_param, proxies=proxy, timeout=(3, 7)).json()
            return ret
        except:
            change_proxy(3)
            return get_ret(c-1)

    ret = get_ret(3)

    since_id = ret['data']['cardlistInfo']['since_id']

    need_list = []
    cards = ret['data']['cards']
    for card in cards:
        if card['card_type'] == 9:
            user_id = card['mblog']['user']['id']  # ç”¨æˆ·id
            user_name = card['mblog']['user']['screen_name']  # ç”¨æˆ·åç§°
            created_at = card['mblog']['created_at']  # å‘å¸ƒæ—¶é—´
            source = card['mblog']['source']  # å‘å¸ƒå·¥å…·

            wb_id = card['mblog']['id']
            wb_url = 'https://m.weibo.cn/detail/' + str(wb_id)

            app_detail_url = 'https://m.weibo.cn/statuses/extend?id=' + str(wb_id)

            def get_detail_ret(c):
                if c < 0:
                    return
                try:
                    detail_ret = requests.get(url=app_detail_url, headers=app_header, proxies=proxy, timeout=(3, 7)).json()
                    return detail_ret
                except:
                    change_proxy(3)
                    return get_ret(c - 1)

            detail_ret = get_detail_ret(2)

            html = detail_ret['data']['longTextContent']  # æ­£æ–‡
            root = etree.HTML(html)
            content = root.xpath("string(//*)")

            reposts_count = detail_ret['data']['reposts_count']  # è½¬å‘æ•°
            comments_count = detail_ret['data']['comments_count']  # è¯„è®ºæ•°
            attitudes_count = detail_ret['data']['attitudes_count']  # ç‚¹èµžæ•°

            pics_url = ''
            if card['mblog']['pic_num'] != 0:
                pics = card['mblog']['pics']
                for pic in pics:
                    tupian = pic['url']
                    pics_url = pics_url + tupian + os.linesep + '   '  # å›¾ç‰‡é“¾æŽ¥

            try:
                video_url = card['mblog']['page_info']['media_info']['stream_url']  # è§†é¢‘é“¾æŽ¥
            except KeyError:
                video_url = ''

            retweeted_status = 'å¦'
            retweeted_url = ''

            if 'retweeted_status' in card['mblog']:
                retweeted_status = 'æ˜¯'
                retweeted_url = 'https://m.weibo.cn/detail/' + str(card['mblog']['retweeted_status']['id'])

            topic_num = int(content.count('#') / 2)  # è¯é¢˜æ•°
            p = re.compile(r'[#](.*?)[#]', re.S)
            topics = '\n'.join(p.findall(content))  # è¯é¢˜

            need = [user_id, user_name, created_at, source,
                    content, reposts_count, comments_count, attitudes_count,
                    pics_url, video_url, retweeted_status, retweeted_url,
                    topic_num, topics,
                    wb_url]
            print(need)
            need_list.append(need)

    return need_list


if __name__ == '__main__':
    while True:
        data = spider()
        save_data('å¾®åš.csv', data)
        print("ðŸ¶ðŸ¶ðŸ¶ðŸ¶ðŸ¶ðŸ¶ðŸ¶ðŸ¶ðŸ¶ðŸ¶")
